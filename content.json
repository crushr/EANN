{"pages":[{"title":"标签","text":"","link":"/tags/index.html"},{"title":"个人简介","text":"大家好，我是钟善男。","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"}],"posts":[{"title":"机器学习 &amp; 深度学习","text":"上周主要对 EANN 的目前研究框架和现状进行了一定的了解，发现了自身缺乏的知识体系。本周通过学习吴恩达机器学习课程和相关网络资源对 EANN 的核心技术：机器学习、深度学习、神经网络进行了系统学习，一定程度上弥补了概念上的短板；同时继续对论文的研究框架进行解读。将一些概念性、形象的、易忘的学习知识记录如下： 机器学习 ML 机器学习分为监督学习和无监督学习，下面对 supervised learning 和 unsupervised learning 进行区分： 监督学习 supervised learning 从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数来预测结果。训练集包括输入和输出，即特征和目标。训练集的目标是人工标注的。 监督学习的最常见的即分类问题：通过已有的训练样本去训练得到一个最优的模型，再利用这个模型将所有的输入映射为对应的输出，对输出进行简单的判断而实现分类的目的，即对未知数据分类的能力。 监督学习的目标往往是让计算机学习我们已经创建好的分类模型。常见技术包括训练神经网络和决策树，常见算法包括回归分析、统计分类，最经典的包括 KNN 和 SVM。 无监督学习 unsupervised learning 输入的数据没有被标记，也没有确定的结果。由于样本的数据类别是未知的，我们需要根据样本间的相似性对样本集进行聚类（clustering），使得类内差距最小化，类间差距最大化。也就是说在实际的应用中，我们无法预先知道样本的标签，即无法得知样本对应的类别。 非监督学习的目标不是直接告诉计算机该怎么做，而是让计算机自身去学习怎么做。PCA 和很多 deep learning 算法都属于无监督学习。 区别 有监督学习方法必须要有训练集与测试样本。在训练集中找规律，而对测试样本使用这种规律。而非监督学习没有训练集，只有一组数据，在该组数据集内寻找规律。 有监督学习的方法就是识别事物，识别的结果表现在给待识别数据加上了标签。因此训练样本集必须由带标签的样本组成。而非监督学习方法只有要分析的数据集的本身，预先没有什么标签。如果发现数据集呈现某种聚集性，则可按自然的聚集性分类，但不予以某种预先分类标签对上号为目的。 运用场景 有训练样本则考虑采用监督学习方法；无训练样本，则一定不能用监督学习方法。但是，现实问题中，即使没有训练样本，我们也能够凭借自己的双眼，从待分类的数据中，人工标注一些样本，并把它们作为训练样本，这样的话，可以把条件改善，用监督学习方法来做。对于不同的场景，正负样本的分布如果会存在偏移（可能大的偏移，可能比较小），这样的话，监督学习的效果可能就不如用非监督学习了。 深度学习 DL 作为机器学习领域中一个新的研究方向，它被引入机器学习 ML 使其更接近于最初的目标——人工智能 AI。 深度学习 DL 人类的神经系统是一个神经 - 中枢 - 大脑的工作过程，是一个不断迭代、不断抽象的过程。这里的抽象和迭代很关键。从原始信号，做低级抽象，逐渐向高级抽象迭代。人类的逻辑思维，经常会使用高度抽象的概念。 在生理学上，从原始信号摄入开始（瞳孔摄入像素 pixels），接着做初步处理（大脑皮层某些细胞发现边缘 edges 和方向），然后抽象（大脑判定，眼前的物体的形状 parts），然后进一步抽象（大脑进一步判定该物体是什么 models）。 总的来说，人的视觉系统的信息处理是分级（分层）的。从低级的 V1 区提取边缘特征，再到 V2 区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。 也就是说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就少，就越利于分类。例如，单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。 Deep learning 的 deep 和多层次体系形成对应。 特征 特征是机器学习系统的原材料，如果数据能被很好的表达为特征，通常线性模型就能达到满意的精度。 特征表示的粒度是很关键的，通常情况下像素级的特征表示方法没有作用，我们需要使特征具有结构性，换言之具有含义，学习算法才能发挥作用。 初级特征表示 稀疏编码（Sparse Coding）：Sum_k (a[k] * S[k]) –&gt; T, 其中 a[k] 是在叠加碎片 S[k] 时的权重系数。（照片组拟合还原照片，视觉实验） 结论：被选中的 S[k]，基本上都是照片上不同物体的边缘线，这些线段形状相似，换言之，一些复杂的图形，往往都可以由一些基本结构组成。 结构性特征表示 高层表达由底层表达的组合而成，底层成为基 basis。 按照初级特征表示，V1 取出的 basis 是边缘，V2 是 V1 层这些 basis 的组合，这时候 V2 又同时作为上一层的 basis，以此类推。 直观来说，就是找到 make sense 的 patch 再将其进行 combine，就得到了上一层的 feature，而后递归向上 learning feature。 上面是从图像角度来说的，如果从文字角度来说，一个人在看一篇 doc 的时候，眼睛看到的是 word，由这些 word 在大脑中自动分词形成 term，再按照概念组织的学习，先验的学习，得到 topic，然后再进行更高层次的学习。 doc 概念 -&gt;topic（千 - 万量级）-&gt;term（10 万量级）-&gt;word（百万量级） 试想，每个人都是从一个受精卵发育而成，但是为什么都会有不同的个性、信仰、生活方式和不同的人生经历呢？ 因为环境造就人，不同的环境就给每个人造就了不同的生活发展方向，既然初始相同（同为受精卵），只是环境不同，那么此处的环境就可以称为特征，是引导不同生活的节点。物以类聚，人以群分，类似的某些环境（特征）导致了类似的性格，也就是环境（特征）使其趋于成了相似的人。 其次，这里的每个人经历的节点可以看作是深度，即层次。各类不同的人群就是不同的深度的表现结果。 越给定一个人的环境（特征），就越清楚这个人是一个什么样的人。但是特征越多，其性格的判定其就会越复杂。每一次的节点的出现都是因为上一次节点的选择结果，是每一次迭代，每一次迭代都会清楚的表示此人的性格。 关于特征数量 任何一种方法，特征越多，给出的参考信息就越多，准确性会得到提升。但特征多意味着计算复杂，探索空间大，可以用来训练的数据在每个特征上就会越稀疏，都会带来各种问题，因而并不一定是特征越多越好。 总结：DL 参考人的分层视觉处理系统，让机器自动学习良好的特征，而免去人工选取的过程，因此 DL 需要多层来获得更加抽象的特征表达。 深度学习的基本思想 在 DL 中，我们需要自动地学习特征，现在假设我们有一个系统 S，它有 n 层（S1,…Sn），它的输入是 I，输出是 O，形象地表示为： I =&gt; S1 =&gt; S2 =&gt; … =&gt; Sn =&gt; O 如果输出 O 等于输入 I，即输入 I 经过这个系统变化之后没有任何的信息损失，当然这是不可能的。保持不变意味着输入 I 经过每一层 Si 都没有任何的信息损失，即在任何一层 Si，它都是原有信息（即输入 I）的另外一种表示。 我们通过调整系统中的参数，使得它的输出仍然是输入 I，那么我们就可以自动的获取得到输入 I 的一系列层次特征，即 S1，… ，Sn。 对于深度学习来说，其思想就是搭建一个堆叠多层的系统，即这一层的输出作为下一层的输入。通过这种方式，就可以实现对输入信息进行分级表达。 另外，前面是假设输出严格地等于输入，这个限制太严格，我们可以略微地放松这个限制，例如我们只要使得输入与输出的差别尽可能地小即可。 浅层学习和深度学习 浅层学习是机器学习的第一次浪潮。上个世纪末，用于人工神经网络的反向传播算法（Back Propagation 算法，简称 BP 算法）发明于世，掀起了基于统计模型的机器学习热潮。人们发现利用 BP 算法可以让一个人工神经网络模型从大量训练样本中学习统计规律，从而对未知事件作预测。这时的人工神经网络，虽然也被称为多层感知机，但实际上只是一层隐层节点的浅层模型。 之后各种浅层机器学习模型被相继提出，例如支持向量机 SVM、Boosting、最大熵方法等，这些模型的结构基本都可以看成是带有一层隐层节点的模型。 深度学习是机器学习的第二次浪潮。Geoffrey Hinton 和他的学生在《科学》上发表了一篇文章，指出：多隐层的人工神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；深度神经网络在训练上的难度，可以通过“逐层初始化”来有效克服，在这篇文章中，逐层初始化是通过无监督学习实现的。 深度学习通过学习一种深层非线性网络结构，实现复杂函数逼近，表征输入数据分布式表现，并展现了强大的从少数样本集中学习数据集本质特征的能力。多层的好处是可以用较少的参数来表示复杂的函数。 深度学习的本质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更多有用的特征，从而最终提升分类或预测的准确性。因此“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于： 1、 强调了模型结构的深度，通常有 5 层、6 层，甚至 10 多层的隐层节点 2、 明确了特征学习的重要性，也就是说，通过逐层特征变换，将样本空间在原空间的特征表示变换到一个新的特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。 深度学习与神经网络 深度学习是机器学习研究中一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像、声音和文本，深度学习是无监督学习的一种。 深度学习的概念来源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。 深度学习的训练过程九、深度学习的的训练过程 使用自下上升非监督学习（即从底层开始，一层一层的往顶层训练） 采用无标定数据分层训练各层参数，这一步可以看作是一个无监督训练过程，是和传统神经网络区别最大的部分，即特征学习的过程。 具体的，先用无标定数据训练第一层，训练时先学习第一层的参数（这一层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），由于模型 capacity 的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第 n-1 层后，将 n-1 层的输出作为第 n 层的输入，训练第 n 层，由此分别得到各层的参数； 自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调） 基于第一步得到的各层参数进一步 fine-tune 整个多层模型的参数，这一步是一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于 DL 的第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果；所以 deep learning 效果好很大程度上归功于第一步的 feature learning 过程。","link":"/2021/07/11/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-7-11/"},{"title":"卷积神经网络中的卷积操作本质","text":"上周我对深度学习、机器学习、神经网络进行了概念上的学习，但是在真正的实践应用上，还是不太清楚，在上面 textCNN 上的一些步骤的过渡产生了疑惑。为了更好的理解论文的第三章，我对 CNN 中的卷积本质和过程进行了学习，过程记录如下： 我在考研的数学科目是数学一，包括了概率论这门课。在概率论中核心之一就是卷积，利用卷积公式可以解决一些较为复杂的概率难题。 在概率论中的卷积公式： 同时也分为一维和二维两种，分别对应的就是一重积分和二重积分，就拿上述公式为例来说，首先积分是一种连续求和的形式 Sum，被积函数 x(p)*h(t-p)。 可以从这么一个角度这么理解：将 x 函数当作为一个系统的输入，当然这个输入是不稳定的；h 函数便是一个系统的输出，这个输出是稳定的，利用卷积公式求得的是一个系统的剩余存量。 函数 函数式 性质 第一个函数 x(p) 不稳定输入 第二个函数 h(t-p) 稳定输出 CNN 的来源在上周的学习中有所提及，模仿人脑神经网络结构识别图片的过程去进行机器学习。而 CNN 中的卷积和概率论中的卷积是否一样，又有什么联系呢。我带着这样的疑问又开始研究：什么是图像卷积操作？ 卷积过程 电脑中的一个个图片都可以看成是由一个个像素点所组成的，其本质其实就是一张表格或者矩阵，一种通俗说法一种数学说法，而这张表格中的每一个点就代表了像素的具体的信息，例如灰度值，RGB 值等等，作为属性展现在图片上面。 利用一个 3·3 的点阵与图像进行一个操作，而这个 3·3 的点阵就被称为卷积核，当然这个卷积核是可以自己设置的，可以是 2·2 或者其他，3·3 比较通用。 卷积核与图像上的点阵进行一一的矩阵相乘。 9 个数相乘之后再进行球和操作，由此可以得到一个新值，也就是一个新的像素点。 利用卷积核将整个图像进行再一次的遍历扫描，由此可以得到一个相比原图像缩小一圈的图像。如果为了使图像在变化前后的大小不变，可以在原图的外围填充一圈全零的像素点，如此的卷积结果得到的图像大小就不会改变。 但是在这整个操作过程中，没有很明显的与概率论中的卷积公式产生对应的部分。但是如果把目光放到整体的卷积过程上，可以发现图像的卷积操作就是将图片与卷积核先相乘再相加，那就可以确定图像和卷积核分别对应卷积公式中的两个函数，其中图像应该对应不稳定的输入量，而卷积核对应的是稳定的输出量 卷积操作本质 第一层理解 将卷积公式放在应用里看。首先有一种耳熟能详的规律：蝴蝶效应（一只南美洲亚马逊河流域热带雨林中的蝴蝶，偶尔扇动几下翅膀，可以在两周以后引起美国得克萨斯州的一场龙卷风。） 如下图，在 x 时刻，蝴蝶扇动了翅膀，而这个行为会对 t 时刻的飓风产生影响，而这个影响会随着时间的变化而发生变化。这个影响力的变化是由 g() 函数来决定的，随时间的变化，这个影响力也在同时发生变化。而在求和后，也就是卷积后就代表着蝴蝶效应的效果。 再抛开具体的例子，可不可以直接抽象为：在某时刻发生了一件事情，而这件事情的产生，它是会受到之前发生的很多事情的影响的，例如在 x 时刻发生了一件事，这件事会对 t 时刻产生影响，具体的影响力还要看从 x 到 t 这段时长，以及影响力本身的变化函数 g()。g 函数就规定了之前发生的事情随时间的影响力的变化。 要注意的是 m 在这个例子中的横纵轴的具体含义也是可以变化的。 第二层理解 再回到之前的在图像上进行的卷积操作，能不能说成： 很多像素点对某一个像素点是如何产生影响的。 有一种卷积操作称为平滑卷积操作，效果是让图片变得更加平滑，更加朦胧。而这个卷积操作中的卷积核就是： 这样的卷积核本质上就是找到一个像素点，把它周围的像素点全部加起来求一个平均值。具体的效果如下： 将原理和效果对应起来看就好理解了，卷积核的平均求和操作能够使得各像素点的属性差异减小，以此就可以达到平滑的效果。 那么回到之前那个提法：图像的卷积操作就是很多像素点对某一个像素点是如何产生影响的。更加具体地说是，卷积核就是规定了某个像素点的周围像素点是如何对该像素点产生影响的。这是在 3·3 卷积核的情况下，其他的卷积核类似。 再和与卷积公式相联系去理解这整个计算过程： 可以发现的是，卷积操作的相乘求和实质上就是原矩阵与卷积核的翻转后（旋转 180 度）的矩阵的乘积求和，因此在卷积公式中的 g 函数对应的矩阵本身不是卷积核，而是 g 函数对应的转置矩阵才是卷积核。 到这里可以简单做一个总结：从卷积到图像的卷积操作其中的关键就是要把卷积当作是过去对现在的影响，周围像素点对当前像素点的影响，而卷积公式中的 g 函数就是规定了如何影响的关键。 那么我们知道，卷积操作是卷积神经网络第一层的关键，那接下来继续学习的就是卷积神经网络第一层到底在干什么，和卷积的关系是什么。 第三层理解 上周的学习可以知道通过 CNN 我们可以进行图像识别，而往往需要识别的一些图像都是复杂的、不规整的，虽然人眼能够一眼分辨，但是计算机识别的都是一个个像素点，可以理解为由无数的数字构成的矩阵，对于一些不规整的图像，计算机是无法通过对应标准图像来进行识别的，如下图： 仔细观察可以发现，两个图像虽然不同，但是局部上是有相同的地方的，因此，卷积神经网络的第一步就是把图像的局部特征给挑出来，再将这些局部特征交给神经网络，由神经网络去判断。 那么如何去提取这些局部特征呢，答案就是对图像进行卷积操作。那么这就可以和之前关于卷积操作的学习对应起来了。 从上面的学习可以知道，图像上的卷积操作就是去处理像素点和周围像素点之前的关系，当卷积核是平滑卷积核时，处理完的结果应该是比原图更加清晰的图片；当卷积核是其他种类时，得到的图片效果大大不同，如下所示： 如果是用上侧卷积核，得到的图像会忽略纵向边界，只会保留横向的边界；用下侧的卷积核时，得到的图像则会忽略横向边界，只保留纵向的边界。这个时候虽然还是进行的卷积操作，但是与平滑卷积核的效果完全不同了，它们只是把图片中的一些特征给挑了出来。 因此，卷积核如果挑选的合适，那它就可以对图片进行过滤，把某些特征给保留下来，而其他的特征则被过滤掉了，这样的卷积核又被称为过滤器，图中上下两个卷积核分别称为垂直边界过滤器和水平边界过滤器。之前，我把卷积操作当作周围像素点对当前像素点的影响，数学上就是相乘再相加的一种形式，又可以理解成为自己对周围像素点的一种试探，而卷积核就是试探的模版。过滤器给人的感觉就是这个像素点在进行主动的试探和选择。 在这种情况下，可以理解为当前像素点对周围像素点的一种主动的试探和选择，通过卷积核把周围有用的特征给保留了下来。 总结 学到这里，其实我对卷积的含义和本质有了更深的认知，最后对卷积含义做一个总结： 第一层：不稳定输入，稳定输出，求系统存量； 第二层：周围像素点对当前像素点如何产生影响； 第三层：过滤器的卷积核规定了一个像素点会如何试探周围像素点，筛选图像特征。 其实这三层意义也分别对了三种应用：信号系统应用、图像处理应用、图像识别应用。","link":"/2021/07/18/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-7-18/"},{"title":"感知机的缺陷","text":"引入感知机的概念 就目前我的知识体系，我知道的是神经网络就是在模拟生物的神经系统，而生物的神经系统都是由一个个神经元组成的，关键的是，这个神经元能够实现多个信号输入，一个信号输出，即多对一。对应的是，人工智能的神经网络也有自己的基本单元，是由一个个感知机组成的，并且其也应该是多对一的关系。 在数学上，函数的映射是多对一的；在数据结构中，树也是一个多对一的结构。那么既然在数学和计算机领域都已经有类似的概念，为什么要单独提出一个神经网络的概念，难道是新瓶装旧酒？神经网络到底有什么特殊之处？这个特殊之处带来了什么样的特殊能力？ 同时我了解到，感知机是由天然缺陷的，人工智能的奠基人之一——马文·明斯基在 1969 年就公布了感知机的缺陷，这一缺陷使神经网络停滞了 30 多年。于是，我对感知机是什么，其缺陷又是什么，后来这个缺陷又是被如何解决的这些问题产生了兴趣，并继续研究下去。 提出感知机的缺陷 通过互联网资源，我找到了一些解决这些疑问的抓手。已知的是生物神经系统的基本单元是神经元，神经网络也有自己的基本单元，称为感知机，这个感知机也实现了多对一的关系，即可以有多个输入，经过一系列计算后，只能输出 0/1 这一种信号。当年感知机被提出后，人工智能的神经网络方面逐渐变成了显学，似乎它可以解决很多的问题。 好景不长，明斯基公布了感知机的缺陷，别说人工智能了，可能连基本的运算对于神经网络都会产生问题。当然随着近年神经网络的火热，说明感知机的优势、劣势以及缺陷都被解决了，对于感知机的学习肯定是一个闭环。 首先，明斯基提出的感知机缺陷指的是，感知机能够处理很多逻辑运算，与、或、非等等这些都能够搞定，但是异或运算是其无法解决的。现在的电子计算机归根到底其实都是逻辑运算，虽然其功能很多，其实根本上都是由门电路实现的与、或、非等等逻辑运算的实现。那么如果感知机无法处理异或运算，那就说明感知机的功能低于计算机，也低于人脑。而当时普遍认为人脑的计算力是大于等于计算机的，因此当时明斯基对感知机缺陷提出了批判。 具化两个问题 一：明斯基说感知机无法实现异或运算，但是能实现与、或、非运算。我们知道的是，逻辑运算就只有三个基本运算，即与、或、非，其他的任何逻辑运算都可以利用这三个组合来实现。那感知机都能够实现与、或、非运算了，那又为什么无法实现异或运算呢？ 二：假设感知机能够实现这些全部的逻辑运算，那其特殊意义在哪里呢，这些功能当时的电子计算机不都已经实现了吗。那在当时感知机被很多人看好的原因一定有其新的特征，新的能力。 解决这两个问题 了解感知机是干什么的 通俗来说，感知机就是一个分类的工具，例如有下表的关于人的身高体重的数据，将这些数据交给感知机，感知机就可以去判断哪些人偏胖，哪些人偏瘦，哪些人身材标准。 人 身高 体重 A 180 78 B 177 60 C 167 58 D 190 80 以上数据是从身高体重两个维度进行判断，实际工作中还可以有更多的维度。那么有了这些数据之后，感知机又是如何判断的呢，感知机的判断标准又是谁来定义的呢。那这就要引入现在人工智能领域常用的训练了。训练的具体算法是需要运用到随机梯度下降算法，这个方法的具体研究暂时放到后头说。 利用二维数据进行解释： 在上图的坐标轴中，虚线以上代表瘦，虚线以下代表胖，蓝点代表瘦的人，黄点代表胖人。随着不断插入更多的训练数据，这根虚线进行需要进行调整来符合胖瘦的划分。等训练用的数据分配完成后，就代表这个感知机完成了。图中的直线即判断一个人胖瘦的标准，训练完了之后任何一个新数据就能根据这个标准来判断胖瘦。 上面描述的是二维的结果，三维同理，划分标准变成了一个平面： 从以上是从理解上来举例说明的，那么从原理上说如下图： 上图的一大部分都可以直观理解我二维过程中的那条直线，最后输出的分类就是在判断输入的数据到底是在这根直线的哪一侧，最后输出的结果只有两类，要么是正，要么是负。 小结：从上面我们可以知道，感知机是用来处理分类问题的，但只能处理二分问题，即非此即彼的问题，分类方式只能用线性进行划分。如果是二维问题，则是一条直线；三维问题，则是一个平面；n 维的问题，则是一个 n-1 维的超平面。 感知机分类的优势 那到这里我在想，分类问题实则就是一个函数映射问题，本质还是多对一，那感知机的分类有什么不同呢？ 对于分类问题，最主要的是需要找到一个分类模版，凡是这类的分类问题，该模版都是有效的，带入数据后调参即可。那么感知机在这时的价值就体现出来了，即上图的模型。只要问题是线性的、二分的，那么都可以按照这个模型实现出来。感知机不仅确定了这个模版是存在的，它还确定了这个模版到底是什么样子的。 如果要进行直接的分类，那么这个函数一定是不连续的，可能是难以用函数来表示的。但是通过感知机，这个问题一定可以表达成一个线性函数再加上一个判断函数（严格来说是激活函数）。那么感知机的价值就体现出来了，通过两个简单的函数就能解决一个复杂的问题；原来没有统一解决方案的问题，变得有统一解决方案了。 虽然感知机表述起来简单，但是背后的数学证明仍是复杂的，不在此过多描述。那么总结下感知机背后体现出来的一个思想：感知机体现的是一种分治的思想，它把一个复杂的、不确定的问题拆分成一些简单的问题，分而治之，最后这些简单的问题利用计算机就解决了。因此可以看出，感知机的确有着自己的独特之处。 接下来，我们从观察公式： 这个公式可以结合上面的模型图一起理解，t 就是输出，1/-1 两种结果，函数 f 就是最后的判断函数（激活函数），那么最重要的就是函数 f 里面的内容。我们知道感知机主要有两个部分，一是判断数据在分界线的哪一边，另外一部分就是分界线的本身； 上式中的左框中即代表分界线的本身（数学上多维超平面的表达式），Wi 可以理解为 n-1 维的超平面的系数，故整体就指的是该超平面的两侧。 上式中的右框即是用矩阵的形式来表示的，向量点积运算，形式上看起来会比左框更加简单。（其实就是为了利用线性代数简便表示） 小结：感知机的优势就在于给了某些分类问题一个统一的模版，我们只需要进行调参数就可以了，这个参数我们通过公式可以知道即是 Wi 这个系数。 感知机的缺陷 观察一个简单的二分问题，前三幅图的与、或、非有关的三种运算都能用一条线进行划分开来；第四幅图表示异或，只有当两值不同的时候才为 1，取值相同为 0。很明显，异或问题上没有办法用一条平面直线区分开来，必须画一个圈才能将 0/1 区分开来。 以上就是明斯基提出的感知机缺陷，即异或运算没有办法可以被线性分割。 缺陷的解决方法 通俗来讲，一个感知机解决不了的问题，那就多用几个。因为异或运算是可以分解成与、或、非运算的，那么进行分解就是解决策略。 如上图，将异或运算分解成一组或运算，前面可以用一个感知机解决，后面也可以利用一个感知机解决，即图中的两个绿点；最后放到另一个感知机即蓝点解决问题。观察这整一个结构，很明显与数据结构中的树结构有明显差异。 再具体探究下，异或问题到底是如何解决的： 上图代表三个感知机在解决问题中的二维分类结果。起决定作用的是前两个感知机的左下角和右上角重复，因此排除掉重复的情况后就能进行线性分类了。实质上就是两个感知机将不可分的点并到了可分的点上。 另外，还有升维的方法进行线性划分，如下图所示： 二维平面无法划分的情况升至三维空间即可进行划分，利用的是 kernel（核方法）这里有可以引入一种核感知机（kernel perception），可以结合 SVM 支持向量机的各种方法。 当然目前感知机仍有一种严重的缺陷：因为感知机是一种在线学习的算法 online learning，因此利用核戏法 kernel trick，需要将计算转化为对偶模式 dual form。这就意味着，在训练数据集的数据量 m 极大的情况下，每次计算的时间复杂度为 O（m2），因此现在的主流算法基本都是各种 SVM 的变体与神经网络了。 整体总结 感知机就是一个分类的模版，一个线性函数 + 一个激活函数，但这个模版的能力有限，只能解决线性的二分问题，异或、非线性的问题感知机就无法解决了，这时候可以利用升维或者变形来解决，或者增加感知机数量来解决。","link":"/2021/07/23/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-7-23/"},{"title":"无限逼近真理的神经网络","text":"神经网络与感知机的比较 从卷积到感知机，再到本节神经网络，距离我的目标掌握 CNN 卷积神经网络越来越近了。从上节可以知道，神经网络的基本单元就是一个个的感知机。 上图就是一个简单的神经网络，图中的小圆圈就是一个个感知机。我们现在知道，感知机主要有两个部分，一个是线性函数，一个是激活函数。当把所有的数据输入一个训练好的感知机以后，这些数据都会被进行二分。其中线性函数就是进行二分数据的标准线，在标准线的每侧都是不同的一类。 然而一个感知机的能力是有限的，他只能对数据进行二分，而且这些数据都必须是线性可分的，那么面对那么多限制的感知机，神经网络就出现了。 如上图就是一个简单的神经网络，输入可以是图片或者一些属性特征。黄色代表输入层；灰色就是感知机，可以称为隐藏层；蓝色就是输出层，代表我们希望看见的结果，可以是一个也可以是多个。 对于这样的神经网络还有一个显著特点：某一层的节点都与下一层的节点全部相连，因此也被称为全连接网络。数据的传递方向也是单向的，它只会朝着神经网络一直向前传递，因此也被称为前馈神经网络。 当然除了全连接和前馈神经网络，也存在其他，如下图： 第一副图中的神经网络经过卷积和池化后结构发生了改变；第二幅图中，数据会在初始节点进行循环，这也就不是前馈神经网络了，称为循环神经网络 RNN。 通过上图的例子可以直观理解一个最简单的神经网络，一张猫的图片输入后在输入层通过不同的特征进行划分，如耳朵、毛、胡子、脸，通过在隐藏层增加感知机，即神经元，可以更好的对输入的特征进行判断，使整个问题的组合可能性更加多元化。而当这些可能性丰富之后，总能够调整到一种状态，能比较清晰的把猫和狗区分开来。因此，一个典型的神经网络是必须具备隐藏层的，而隐藏层的具体个数等信息是需要根据具体情况进行调整的。 比起感知机，神经网络的优势也就体现出来了。比起感知机只能处理线性的、二分的问题，神经网络能够处理更加丰富的问题。 小结 普遍逼近定理：神经网络只要有一个隐藏层，那它就可以逼近任意一个连续函数。其实类似于数学上的傅里叶级数，傅里叶级数将一个复杂的函数拆解成一个个圆周运动（正弦波）；神经网络也是把一个复杂函数拆解成一个个感知机，即一个线性函数 + 一个激活函数。如上所说，神经网络的优势已经很明显了，但是劣势同样不可忽视。一个全连接的神经网络直接进行运算的话，算量过大，需要被调节的参数过多。我们现在知道的解决方法有梯度下降法、随机梯度下降法、卷积 + 池化来降维。这些方法的学习先暂存一下。 提出问题 在上节感知机的学习中可以知道，在训练感知机的时候会首先输入一堆数据，这些数据可以被分为两类，通过这两类数据进行夹逼，反向形成一条分界线。当分界线确定后，训练也就代表完成了。 但是细想神经网络，与感知机仍是有差异的，举个例子，如果要让神经网络识别出猫，那么需要提供很多猫的照片让神经网络进行训练，但是不是猫的照片种类实在太多了，这是无法实现提供的。换言之，只能提供肯定一侧的数据，无法给否定一侧的数据，而这个特征就决定了神经网络无法像感知机一样利用两侧的数据将那条分界线夹逼出来了。 解决问题 在吴恩达的机器学习中，从感知机到神经网络的课件表示中将激活函数从 0/1 跃阶函数换成了 sigmoid 函数。 Sigmoid 函数如上所示，与 0/1 跃阶函数相比，就是将一个离散的跳跃过程换成了一个连续的过程。 在感知机的学习中可以知道感知机可以用来很好的解决线性可分的二分问题，都可以拆成一个线性函数 + 一个激活函数。 激活函数 以上可以理解为感知机的形式上的价值，为了方便理解，将感知机的两部分赋予现实的意义，其中线性函数可以理解成对某个类型里的标准模型的描述，激活函数可以理解成一个判断的标准，它在判断输入的数据符不符合这个标准。其实神经网络也是一样的，只不过神经网络是需要经过多次的描述 + 判断的，每经历过一个神经元就要进行一轮的描述 + 判断。 针对具体问题，往往一轮的描述 + 判断是无法解决实际问题的，例如图像识别等等。那这个时候就能体现神经网络的价值了，神经网络会用自己的方式去描述猫的标准模型，实质上就是一堆的线性函数 + 一堆的激活函数，其中所有线性函数的集合就是对猫的标准模型的一个描述，而所有激活函数的集合就是在判断输入数据是否符合该描述的模型。神经网络就是靠着层层的“逼问”，最后把猫是什么给描述清楚了。 因此在某个层面上，神经网络和感知机的作用都是将复杂的、难以用人的理性进行描述的问题描述出来，而且通过增加神经元的方式使这个描述无限的逼近真相。 如果说线性函数为神经网络提供了描述世界的世界观的话，那激活函数就相当于为神经网络赋予了价值观。因此，当把激活函数从原来的 0/1 跃阶函数换成了 sigmoid 函数，这不仅仅是一次从原来的粗糙变精细的量变，而是一个更换最根本问题的制品。当把 0/1 跃阶函数换成了 sigmoid 函数，就相当于把最基本的问题从原来的对错 / 是非换成了好坏这样的问题，定性变成了定量。 基本问题换成了好坏问题，那就不能进行简单的判断了，而是要能反映出具体的程度。 猫与猫神 例如给出了 GitHub 的一个 logo 图，我们不能简单地去判断这是不是猫，而是要去判断，这有多像猫。至于有多像，也是需要一个标准去进行比较的。 在分类学中，有一个模式种的概念，能够被比较的标准模式只有一个，都以其为基准。那么现在我们要去判断上图中的图到底有多像猫，也是需要提供一个模式种来进行比较，拿图和模式中进行比较才能知道图和猫有多像，那么这就是问题所在。 我们之所以用神经网络就是因为我们搞不清楚什么是标准的猫，就想用神经网络来解决什么是标准的猫这个问题。那么现在问题又回来了，我们要把神经网络训练好了，就得先把猫的标准定下来，这就进入了一个逻辑循环。 当然这个问题已经被解决了。我们之前遇到的所有的问题的来由，不就是因为在真实世界中挑不出一只标准的猫用来代表所有的猫吗，那就没有必要在真实世界中找了，而是在理念世界中抽象和创造出一个标准的、完美的、不可言说的“猫神”作为模型。真实世界中的所有的猫都是理念世界中猫神的具象化。所有可以描述出来的猫的模型也都是猫神的近似表达。 那我们利用神经网络找到的猫的模型肯定也是猫神的一种近似表达。那么猫神模型如何确定呢，那其实是不需要确定的，关键是提前把训练用的图片准备好，再将这些图片打上标签，进行认证，凡是打了标签的就认可这是猫，利用实例猫来逼近猫神模型再将其作为标准。那神经网络中的那些模型只是猫神模型的近似。 然而，用神经网络去判断这些被打了标签的图片，那肯定多少会有一些偏差。有的模型偏差大，有的模型偏差小。这个偏差距离对应着了实例猫和猫神的差距大小，越小越好。训练神经网络就是要把那个偏差最小的模型给找出来，这里面存在一系列数学推导过程，暂存下次再研究。 geogebra 简单模拟三层神经网络模型 这里是 geogebra 简单模拟的一个三层神经网络模型： 我们可以通过调整参数来改变右侧模型的形状，以此改变最终的分类结果，而且激活函数非常重要，如果没有激活函数，图像则会是这样的： 当只有激活函数发生作用时，输出结果才会有高度的变化和跃迁；如果没有激活函数，会造成隐藏层坍塌，整体仍是二分线性的，多层意义失效，故而图像则只会是平面，哪怕经过了很多的神经元感知机，都只能是平面的形式。 可以联想下数学中的傅里叶级数，傅里叶级数是利用正弦函数去逼近任何一个波形，而神经网络是利用感知机去逼近任何一个统计模型。因此机器学习也被称为统计学习。 约翰·霍兰德遗传算法研究 遗传算法之父约翰·霍兰德基于元胞自动机和遗传算法做过一个研究，如下图： 有一个类似棋盘的表格，上面分布着很多的豆子，和一个小人。这个小人有一套指令集（向前走、向左走），这个小人每走一步吃到豆子就会加分，没吃到豆子就会减分。霍兰德就想找到一个策略，让这个机器人能吃到最多的豆子，但是移动的步最少。利用遗传算法最终找到了最优策略。这个实验的结论非常有意思：这个最优策略无法被归约，意思就是这个实验的最优策略没有办法被固定下来，只能按照特事特例的方式来呈现为一个大表。 那为什么这个实验的策略无法被归约下来，而真实世界的物理规律可以被统一表示呢？那其实可能是这样的，这个策略是演化的产物，进化是要优胜劣汰的，中间的每一个结果都是要进行生存还是毁灭的价值判断；而物理规律是在发展，而非在演化，它只是把底层规律进行了组合，并不需要谁去判断这个规律组合的好坏与否，并无价值判断。 总结 在前面，我已经知道了激活函数的重要性，把它类比于一种价值观。如果没有激活函数，只有线性函数，那么无论线性函数嵌套多少层，它依旧都是一个简单的线性函数，都能被统一表示。但是一旦有了激活函数，有了价值判断，那其描述的事情就没有办法去被简单的表示了。 回想到猫神的例子，小时候我们如何学会辨认什么是猫，父母或者老师指着一张猫的图片告诉我们这是猫，但是随着我们的成长，我们慢慢认识越来越多种猫，在我们的心智空间中，不断为猫开辟出特例，而这些事物往往是不能被统一表示的，因此这也是认识中的一段演化的过程，但是肯定的是，我们在无限逼近真相。 实质上以上的例子就是人脑的经验学习，那么我们在寻找真相的过程中，也许神经网络这种无限逼近真相的范式已经是最好的范式。 在本节中的学习中，我发觉万物都离不开数学和哲学，神经网络也不例外，用数学去解释它，再用哲学去理解它，这真是一种奇妙的感觉。","link":"/2021/07/24/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-7-24/"},{"title":"损失函数 - 最小二乘与极大似然估计","text":"引入 在了解学习完神经网络之后，下一步我要学习的就是梯度下降法了。因为梯度下降法就是训练神经网络的基本方法。然而，在了解梯度下降法之前，我又碰到了一个新概念——损失函数。 所谓的梯度，就是损失函数的梯度。在掌握损失函数如何计算之前，我心中一直有一个困惑：损失函数到底是如何设计出来的呢？在我了解之后，明白至少有三种方法： 1、 最小二乘 2、 极大似然估计法 3、 交叉熵法 因此，在我了解梯度下降法之前，弄清楚这三种方法是有必要的。 神经网络（监督学习） 在神经网络的学习中我明白了神经网络无非是由一堆线性函数 + 激活函数，形式简单但能解决很多复杂问题。 美国的联邦大法官斯图尔特经常被法律界之外的人提起，他被提起不是他的法律程序而是在认定色情这件事儿上，他曾说过：“色情是什么，我不知道，但是你拿给我看，我就能判断出来。“ 神经网络就是起着反向的作用，它知道我说不清楚猫到底是什么，没关系，但是我只需要看照片，回答是还是不是，它就能反向推出我心中的标准具体。 首先，神经网络要先认定在我的心中是有一个完美标准，然后它会在自己的心中也建立一个标准用自己的标准和我的标准进行比对。它的目的就是调整自己来找到与我最接近的那个标准（拟合）。但是具体实践起来还是有障碍的，因为我也说不清自己的标准，那就没办法一条一条的拿出来跟它比对。 当然，这也是有解决办法的。在我的心智空间和神经网络的空间两者之间是有接口，这个接口就是我可以去判断那些照片到底是猫还是不是猫，我的判断这个结果就可以拿给神经网络。然后神经网络拿着同样的照片也去用自己的标准去判断，判断完成后再用这个结果和我的结果进行比对，比对结果相差越小，就能够证明它的这个标准和我的标准越接近。而神经网络再根据这个差距来调整自己的过程，就是在学习、训练的过程，而我提前判断好的图片的结果，就是用来训练的数据。 吴恩达的机器学习课程中是讲过损失函数的，大致意思就是：损失函数就是神经网络里的标准和我心中的那个标准相比较相差多少的定量表达。具体的表达式吴恩达写了两种，如下： 红色框中的分别是两个损失函数的具体表达。虽然式子是给出来了，但是他并没有说明为什么要这么写，依据是什么。 于是我尝试去探索一下。在神经网络的学习中，我知道了猫的标准其实可以看做是一个统计模型。我们人脑里面的那个是绝对正确的概率统计模型，神经网络里面的那个是需要学习、需要调整的概率统计模型，关键是如何把他们两个进行比较，这是令人疑惑的。 因此弄明白为什么损失函数的表达式可以有两种写法，这两种写法的依据又是什么是很有必要的。根据很多网络资源和解答视频，可以发现三个关键词： 1、 最小二乘法 2、 极大似然估计法 3、 交叉熵法 而这三个词正是去比较两个模型（人脑；神经网络）之间差距大小 3 种思路，所以接下来我就对每一种思路进行深入的研究。 最小二乘法 其实比较两个概率模型之间的差别最简单的方法就应该是最小二乘法。因为人脑里的那个概率模型现在我也讲不清楚，神经网络经过感知机的嵌套之后的模型我也讲不清楚，那在这种情况下，最简单的方法就是让两者直接去对比判断的结果。比如给一张照片，人脑和神经网络分别去判断他到底是不是猫，到底有多像猫，然后再进行判断。 现在就用一个神经网络来具体看一下这个处理的过程。 上图就代表神经网络处理猫照片的过程。 首先，这些猫的照片会经过人的认定即打上一个标签，这个标签的结果 Xi 中的下标 i=1 代表是猫，i=0 代表不是猫。同样这个照片会输入到神经网络，经过神经网络判断之后才会输出一个结果，这个结果就 Yi 就是 sigmoid 函数，它的范围就在 0 到 1 之间，这张照片经过神经网络以后，神经网络得出的结论是这一张照片有多大的概率是猫。同时，神经网络也有自己的参数体系，W 对应的是感知机的线性函数的未知数的系数集合，b 对应的是线性函数的偏置系数集合。 回过头来说，我们希望去比较人脑中去判断猫的模型和神经网络中判断某种模型到底相差有多大。那么最符合直觉的方式就是直接去比结果，即人脑判断结果和神经网络判断结果的差值，这个差值越小就代表人脑和神经网络中的模型相差也是最小。 因此在理解上，这个差值｜Xi-Yi｜就能够代表判断结果差距，然后为了避免绝对值情况，同时也为了可导，数学推导上往往会在这个差值上加个平方变成（Xi-Yi）2，之后再进行求和去最小操作，公式形式就变为了： 观察这个公式就可以理解它为什么叫最小二乘法了【min：最小；平方：二乘】，由此吴恩达写出的第一个式子对损失函数的表达就理解了。 第一个红框中是利用最小二乘法来计算损失函数的式子，公式中的参数形式虽然不同，但本质上是相同的，而 1/2 是为了便于后面求导相消，在图中没有连加符号是因为进行训练的时候是数据逐个输入，然后再用别的方式去找到这个最小值，可以用连加求和取最小来理解。 理解到这里，我就能明白了第一个式子就指的是最小二乘法，也是最简单的一种计算损失函数的方法。然而吴恩达在课程里讲到，如果用它去作为损失函数进行梯度下降法的时候，会显得特别麻烦，所以不建议选用。具体麻烦在哪可以之后再研究，那么现在我就只好把目光移到另外两种方法上。 极大似然估计法 下面利用一个例子来解释这个方法： 想象一下，有两个世界，一个是理念世界（黄色区域），一个是现实世界（绿色区域）。在理念世界中有一个 0-1 概率分布，取到 0/1 的概率对等；在现实世界中存在一个抛硬币的行为，抛十次硬币中五次正、五次反。 但是两个世界是存在一定关系的，即： 1、 理念世界指导现实世界。如果确定了抛硬币的概率分布是 0-1 分布，那么抛硬币的结果在理想情况下就是两种情况等分。 2、 现实世界反映理念世界。如果我们抛了十次硬币，五次正、五次反，那么在理想情况下，就能认定抛硬币的概率分布是 0-1 分布。 但是，理念世界与现实世界并非直接相连的，为了打通这两个世界时，就会出现很多概率统计问题。比如说，现在确定在理念世界中抛硬币的概率分布就是 0-1 分布，但是现实世界的实际情况不一定会出现完美的正反情况等分；反之，如果抛硬币并没有出现完美的正反情况等分情况，同时在理念世界中也对应着一个概率模型，但是这个概率模型并不能用现实情况直接推出，也就是说理念世界中的概率模型并不确定。 现在我们假设有很多模型，即很多种概率分布情况，且现实世界的情况固定时，在某一种概率分布的条件下，发生现实世界的情况的可能性称为似然值。因此，极大似然法就是在挑选出似然值最大的概率模型，就越接近本来的概率模型。我们在已知事情发生结果的情况下，需要去反推产生这个结果的概率模型的时候，往往就会用到极大似然估计法。 那么回到神经网络，我们想用神经网络里的概率模型去逼近人脑中的概率模型和这个过程就非常相似了。在训练神经网络的时候需要提供很多图片，这个图片不就像是抛出来的一枚枚硬币吗？极大似然估计的本质上就是在去计算神经网络中的概率模型的似然值，然后找到那个极大似然值，而那就正是最接近真实情况的那个概率模型。 在抛硬币这个例子里面，硬币落到地面叫现实。 在判断猫的图片的例子里面，人的判断就是现实。 利用猫的例子来推导的极大似然估计法的公式不在此展示。","link":"/2021/07/25/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-7-25/"},{"title":"多模态虚假新闻检测 初探索","text":"本周从柳老师处接过一个新课题——多模态虚假新闻检测，接过这个课题的时候我其实就在思考其意义，因为本科毕设是围绕网络舆情来做的，以微博百万数据做支持，通过分析主题和情感演化趋势来判断舆情传播特征及规律。而虚假新闻这个主体不同的是，它是在文本内部作一个分类，但是这个“虚假”性如何去判断，如何划分一篇新闻是真实的还是虚假的，这个划分标准又在哪里；另外多模态这个概念我也是不明确的，因此我当时头脑中冒出很多疑问。 问题解答 据统计 18 年国际期刊《科学》指出，在 2016 年美国总统大选期间，平均每个选民每天要接触 4 篇假新闻，研究认为这些虚假新闻甚至影响了选举的结果。互联网虚假信息正在威胁着全球互联网的安全，如何快速的辨别出虚假新闻，成为一项挑战。 新闻的多模态内容，即包括文本、配图、用户特征等形式，根据这些内容来判断该新闻属于虚假新闻还是真实新闻。 虚假新闻的类型都不同。有的虚假新闻篡改了图片，有虚假新闻会对图片进行错误解读，还有的虚假新闻是将以前的图片拿出来充当当前新闻的配图。 比赛方向 同时，我关注到一些虚假新闻检测挑战赛[1]，这些比赛的方向大致分为三类： 1、给定一个新闻事件的文本，要求参赛者判定该事件属于真实新闻还是虚假新闻； 2、给定一张图片，要求参赛者判断该图片属于虚假新闻图片还是真实新闻图片； 3、给定一条新闻的多模态内容（包括文本、配图、用户特征等），要求参赛者判断该新闻属于虚假新闻还是真实新闻。 上述三条分别对应着文本单模态、图片单模态、图文多模态，我对每个赛题的得分第一的团队解决方案进行了深入了解，每个解决方案背后都有一个完善的模型支撑。 虚假新闻文本检测（单模态）解决方案： 目前虚假新闻识别领域中“数据驱动加知识驱动”的主流研究方向：BERT 预训练模型加强特征校正；科大一团队提出基于 BERT 和 CNN 的多模型虚假新闻分类，将这个任务抽象为 NLP 领域的文本二分类任务，采用多种结构进行融合，在输入上引入字词结合的形式，充分利用假新闻的关键词特征进行优化。 他们在每一个模型的基础上，进行 10 折交叉验证，然后利用假新闻的关键词特征进行优化。对于训练集中的所有假新闻，利用 textrank4zh 对每条新闻文本取 10 个关键词，汇集所有的关键词，得到前 100 个出现最多的关键词。通过观察这些关键词，发现假新闻喜欢对部分人名、地名、名词、动词进行造谣。 同样对测试集中的每条新闻文本取 10 个关键词，汇集所有的关键词，得到前 100 个出现最多的关键词。对这些关键词先去除在训练集中出现较多的，然后按照人名、地名、名词、动词的方式获得以下几类词： 人名：小泽征尔、翁帆、崔永元； 地名：中国、美国、南京、上海、杭州、福建、晋江； 名词：洪水、新闻、遗产、同学、校车、大学、国家； 动词：视察、证实。 因此，某一新闻的前 10 个关键词含有以上这些词时，它有为假新闻的倾向，因此，在模型融合时可以降低真假新闻的分界线。利用这个关键词特征可以发现更多的假新闻，使假新闻评判效果更好 虚假新闻图片检测（单模态）解决方案 第一名的团队通过特征工程的方法，研究了图片的基本特征、图片中的文字特征、PCA 和 SVD 降维特征，以及 DTC 特征等。 基本统计特征：图片尺寸；图片后缀类型；图片模式（RGB、灰度等）；清晰度、亮度；直方图分布特征；各通道的均值方差等统计特征。 特征意义：关键特征包括图片尺寸和清晰度特征；图片尺寸可以识别图片的来源，比如手机截图的尺寸和相机照片尺寸截然不同。 一般认为图像越清晰越是真的，因为图像经过 ps 篡改之后清晰度会下降，还有一种可能性是谣言往往传播得更快，传播过程中的每一次保存和发送都可能会降低清晰度。 降维特征可以在保证维度正常，提取出表征该图片的关键信息。 虚假新闻多模态检测解决方案 冠军团队“Qingbo&amp;bird”通过提出了一个基于 Gdbts-DenseNet-Bert 联合抽取特征的识别模型，实现了准确全面的多模态识别。 面对 38471 条训练样本，他们通过 Python 对原始特征数据以及构造的特征进行了数据分析。 多媒体新闻主要包含三类特征：一个方面是图像特征，训练数据中含有图片的样本占 80% 以上，一个方面是文本特征，还有一个方面是多媒体新闻的发布或者转发者的用户信息特征，比如粉丝数目、关注数、用户简介等用户画像特征。 “Qingbo&amp;bird”团队使用了 GDBT-based 的模型，针对图像特征，将 densent121 预训练模型的最后一个全连接层的输出作为图像的语义特征。针对 text 文本字段，他们利用 tfidf 提取 ngram 特征。 最后，他们把图像、N-Gram 和 Bert 提取的文本特征、用户画像特征拼接到一起，输入 GDBT-based 模型，训练了一个虚假分类的虚假新闻判断模型。 这个模型框架和论文中（EANN 神经网络模型）的一致，研究明白这个框架是极为重要的，下面对该研究框架进行详细说明。 论文框架研究 Event Adversarial Neural Networks(EANN) 对抗事件神经网络 EANN 的框架流程主要分为两条线和三个组件，线其一是文本，线其二是图像，组件分别是多模态特征提取器、虚假新闻检测器和事件鉴别器，分别展开： 多模态特征提取器 多模态特征提取器：对应多模态，数据源包括文本和图像，提取器包括文本特征提取器和视觉特征提取器，作用是对文本和视觉潜在特征进行深度学习，最终连接在一起形成最终的多模态特征表示。 1、分词后的文本。 2、Word Embedding 词嵌入；将单词所属的空间映射到 Y 空间的多维向量，即找到一个映射或者函数，生成文本在一个新的空间上的表达。作用就是进行文本向量化，在我毕设 LDA 与 word2vec 的部分也运用过，NLP 的前期工作都需要向量化的工作，并且这个向量可以通过神经网络的方式来学习更新。 3、Text-CNN 文本分类，通过一维卷积来获取句子中 n-gram 的特征表示。我在本科毕设中使用 LDA 和 doc2vec 进行的固定维度特征向量提取分类，而 Text-CNN 是采用深度学习的方法，在文本分类中效果比 LDA 和 doc2vec 更好。 TextCNN 对文本浅层特征的抽取能力很强，在短文本领域如搜索、对话领域专注于意图分类时效果很好，应用广泛，且速度快，一般是首选；对长文本领域，TextCNN 主要靠 filter 窗口抽取特征，在长距离建模方面能力受限，且对语序不敏感。 总结 学习到这里我深感自己关于深度学习和社交网络的知识储备缺乏，在很多地方会产生原理性的问题，于是我在 coursera 上开始学习吴恩达的机器学习课程，并完成测验。 本周主要对课题的目前研究框架和现状进行了一定的了解，发现了自身缺乏的知识体系，开始计划搭建体系；下周计划在学习机器学习课程的过程中，开始深入阅读几篇文献，后续尝试撰写文献综述。","link":"/2021/07/04/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-7-4/"}],"tags":[{"name":"神经网络","slug":"神经网络","link":"/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"卷积操作本质","slug":"卷积操作本质","link":"/tags/%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E6%9C%AC%E8%B4%A8/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"感知机","slug":"感知机","link":"/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"},{"name":"损失函数","slug":"损失函数","link":"/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"name":"最小二乘法","slug":"最小二乘法","link":"/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/"},{"name":"极大似然估计","slug":"极大似然估计","link":"/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"},{"name":"交叉熵","slug":"交叉熵","link":"/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/"},{"name":"多模态虚假新闻检测","slug":"多模态虚假新闻检测","link":"/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%99%9A%E5%81%87%E6%96%B0%E9%97%BB%E6%A3%80%E6%B5%8B/"},{"name":"TextCNN","slug":"TextCNN","link":"/tags/TextCNN/"},{"name":"激活函数","slug":"激活函数","link":"/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"},{"name":"猫神模型","slug":"猫神模型","link":"/tags/%E7%8C%AB%E7%A5%9E%E6%A8%A1%E5%9E%8B/"}],"categories":[{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"多模态虚假新闻检测","slug":"多模态虚假新闻检测","link":"/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%99%9A%E5%81%87%E6%96%B0%E9%97%BB%E6%A3%80%E6%B5%8B/"}]}
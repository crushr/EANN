<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>损失函数 - 交叉熵 - 勿以浮沙筑高楼</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="勿以浮沙筑高楼"><meta name="msapplication-TileImage" content="/img/logo.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="勿以浮沙筑高楼"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="通过上周的学习，损失函数的终极目标就是找到神经网络中的模型和人脑中的模型最为相似的那个，目前的实现手段就是找到一个方法，能把人脑中的模型和神经网络中的模型进行对比（定量比较）。"><meta property="og:type" content="blog"><meta property="og:title" content="损失函数 - 交叉熵"><meta property="og:url" content="https://crushr.github.io/2021/08/01/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-8-1/"><meta property="og:site_name" content="勿以浮沙筑高楼"><meta property="og:description" content="通过上周的学习，损失函数的终极目标就是找到神经网络中的模型和人脑中的模型最为相似的那个，目前的实现手段就是找到一个方法，能把人脑中的模型和神经网络中的模型进行对比（定量比较）。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://crushr.github.io/img/3.jpg"><meta property="article:published_time" content="2021-07-31T16:00:00.000Z"><meta property="article:modified_time" content="2021-08-16T13:32:39.297Z"><meta property="article:author" content="钟善男"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="损失函数"><meta property="article:tag" content="最小二乘法"><meta property="article:tag" content="极大似然估计"><meta property="article:tag" content="交叉熵"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/3.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://crushr.github.io/2021/08/01/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-8-1/"},"headline":"勿以浮沙筑高楼","image":["https://crushr.github.io/img/3.jpg"],"datePublished":"2021-07-31T16:00:00.000Z","dateModified":"2021-08-16T13:32:39.297Z","author":{"@type":"Person","name":"钟善男"},"description":"通过上周的学习，损失函数的终极目标就是找到神经网络中的模型和人脑中的模型最为相似的那个，目前的实现手段就是找到一个方法，能把人脑中的模型和神经网络中的模型进行对比（定量比较）。"}</script><link rel="canonical" href="https://crushr.github.io/2021/08/01/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-8-1/"><link rel="icon" href="/img/logo.png"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/github-gist.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo1.png" alt="勿以浮沙筑高楼" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives/">归档</a><a class="navbar-item" href="/categories/">分类</a><a class="navbar-item" href="/tags/">标签</a><a class="navbar-item" href="/about/">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="AlphaLxy GitHub" href="https://www.github.com/crushr"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/3.jpg" alt="损失函数 - 交叉熵"></span></div><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>损失函数 - 交叉熵</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-08-01</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-08-16</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="level-item"><i class="far fa-clock"></i> 13 分钟读完 (大约1930个字)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><div class="content"><h2 id="引入"><a href="# 引入" class="headerlink" title="引入"></a> 引入 </h2><p>　　在上周学习神经网络的损失函数相关知识时，我发现有三个基本的思路：<br>　　1、    最小二乘法 <br>　　2、    极大似然估计法 <br>　　3、    交叉熵 <br>　　在上周学习极大似然估计法时，最后推导出来了一个公式：</p>
<img src="/20210801/1.png" class="">
<p>　　匪夷所思的是，利用极大似然估计法推导出来的损失函数在形式上与交叉熵的形式相像，那么问题就来了：交叉熵到底是什么呢？损失函数的交叉熵和极大似然估计法之间有什么区别和联系呢？这次就先解决这些问题。<br>　　通过上周的学习，损失函数的终极目标就是找到神经网络中的模型和人脑中的模型最为相似的那个，目前的实现手段就是找到一个方法，能把人脑中的模型和神经网络中的模型进行对比（定量比较）。</p>
<h2 id="熵的概念"><a href="# 熵的概念" class="headerlink" title="熵的概念"></a> 熵的概念 </h2><p>　　那么第三种方法交叉熵，其实就是利用熵这个概念，先把模型换成熵这么一个数值，然后再用这个数值去比较不同模型之间的差异。<br>　　利用熵作为中间量进行比较的原因——比较两个模型之间的障碍：如果两个模型是同一种类型，可以对参数直接比较；但如果两个模型不一样，就没有办法进行直接比较，即没有办法进行公度。<br>　　谈及公度，在不同的范式，不同的价值体系之下，货币体系是解决公度问题最好的例子。不论是什么东西，只要放入货币体系里面，那么就会变成一串数字被衡量出来。<br>　　类比现实中的货币体系，如果能找到一个神经网络中关于概率模型的解决方案，那就能使不同类型的概率模型进行比较（公度），这就是熵。<br>　　熵，代表的是一个系统里的混乱程度（不确定性），人脑里面有一个概率模型，神经网络中也有一个概率模型，两个概率模型之间是有一定的混乱程度的。</p>
<h2 id="从信息量推导熵"><a href="# 从信息量推导熵" class="headerlink" title="从信息量推导熵"></a> 从信息量推导熵 </h2><p>　　熵在信息论和热力学中都有出现，从信息论的角度出发，<br>　　前置概念（信息量）：是否能为人消除不确定性【信息是对不确定性的消除】且每条信息中的信息量也是不一致的。<br>　　信息量：一个事情从确定到不确定的难度有多大，信息量越大代表难度越高；单位比特 <br>　　熵与信息量类似，熵指的是一个系统从原来的不确定到确定难度有多大 <br>　　熵和信息量都是衡量难度，单位都是比特 <br>　　信息量 f(x)：</p>
<img src="/20210801/2.png" class="">
<p>　　关于信息量 / 熵的一个例子：</p>
<img src="/20210801/3.jpg" class="">
<p>　　几个重要因素——赢球概率 / 信息量，左右两边代表两个不同的系统（系统 1、系统 2）（阿根廷对比利时、法国对中国）<br>　　已知的是，熵与信息量类似，熵指的是一个系统从原来的不确定到确定难度有多大，那么很明显的是系统 1，即阿根廷对比利时的结果更加难以确定，故而系统 1 的熵应该更高；系统 2，即法国对中国的结果较容易确定，故而系统 2 的熵应该更低。<br>　　值得注意的是，熵是在整个系统的范畴下进行讨论的，故应该用概率作为权重对信息量进行加权求和，这样就能得到对系统贡献的信息量。<br>　　在数学上，加权求和就是数学期望的形式。可以这么说，在一个系统中，熵就是信息量的期望。因此就有下面的式子。</p>
<img src="/20210801/4.jpg" class="">
<p>　　已知有一个概率系统 P，它的熵 H(P) 就是该系统的信息量的期望，同时信息量的函数也是已知的，那么就可继续推导熵的展开式。【通俗来讲，一个系统的熵就是将系统内所有可能发生的各事件的信息量与对应的概率相乘再求和。】<br>　　通过熵的定义可以发现，熵的确是对整体的概率模型进行了一个衡量，衡量的结果可以反映出概率模型不确定性的程度 / 混乱程度。</p>
<h2 id="由相对熵 -KL 散度解释交叉熵为什么能代表损失函数"><a href="# 由相对熵 -KL 散度解释交叉熵为什么能代表损失函数" class="headerlink" title="由相对熵 /KL 散度解释交叉熵为什么能代表损失函数"></a> 由相对熵 /KL 散度解释交叉熵为什么能代表损失函数 </h2><img src="/20210801/5.jpg" class="">
<p>　　相对熵指的不是一个概率系统的概念，如上图存在两个概率系统 P、Q，f(x) 代表信息量；DKL(P||Q) 代表 KL 散度，其中 P 与 Q 不等价，P 在前代表是以 P 为基准的，考虑 Q 和 P 相差了多少。形式上来说，系统 Q 的信息量与系统 P 的信息量之差的整体的期望就是相对熵。<br>　　继续推导公式，可以得到三式，可以发现三式中的第二个求和部分就是 P 的熵，且现在是把系统 P 作为基准（固定不变），因此三式中的第一个求和部分决定了 Q 与 P 的差距，而这个部分就是交叉熵——H(P,Q)。<br>　　根据吉布斯不等式可知，KL 散度一定是大于等于 0 的，故在 Q 与 P 不相等时，一定大于 0，即交叉熵一定大于 P 的熵，且 P 的熵一定大于 0。因此如果要使 Q 的概率模型和 P 的概率模型非常接近，即 KL 散度尽可能趋紧 0 时，就要找到交叉熵的最小值。因此交叉熵本身就能作为损失函数，交叉熵越小代表两个概率模型越接近。</p>
<h2 id="将交叉熵应用到神经网络中"><a href="# 将交叉熵应用到神经网络中" class="headerlink" title="将交叉熵应用到神经网络中"></a> 将交叉熵应用到神经网络中 </h2><img src="/20210801/6.jpg" class="">
<p>　　如上图，交叉熵比较的两个主体对应到神经网络中即 xi 与 yi，其中 xi 是离散的 0-1 分布【0 代表不是猫，1 代表是猫】，yi 则是一个连续函数，反映的是有多像猫的程度，并没有反应多不像猫，故 yi 在推导时需要分成两种情况，在 xi=0 时即人脑判断出不是猫的时候，应该对应的是神经网络判断出的不像猫的概率，故此时应该代入 1-yi，则得到最后一个红框中的公式：</p>
<img src="/20210801/7.png" class="">
<p>　　容易发现，这个式子与极大似然估计法推导的公式是一致的。现在知道，想要找到两个概率模型最接近的那一个，那就等价于要求交叉熵最小的那一个，因此交叉熵和极大似然估计法形式上殊途同归。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>损失函数 - 交叉熵</p><p><a href="https://crushr.github.io/2021/08/01/周报告-钟善男-2021-8-1/">https://crushr.github.io/2021/08/01/周报告-钟善男-2021-8-1/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>钟善男</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-08-01</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-08-16</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="fab fa-creative-commons"></i> <i class="fab fa-creative-commons-by"></i> <i class="fab fa-creative-commons-nc"></i> <i class="fab fa-creative-commons-sa"></i> CC BY-NC-SA 4.0</a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络, </a><a class="link-muted" rel="tag" href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">损失函数, </a><a class="link-muted" rel="tag" href="/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/">最小二乘法, </a><a class="link-muted" rel="tag" href="/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/">极大似然估计, </a><a class="link-muted" rel="tag" href="/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/">交叉熵 </a></div></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/08/02/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-8-2/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">从反向传播到梯度下降法</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/07/25/%E5%91%A8%E6%8A%A5%E5%91%8A-%E9%92%9F%E5%96%84%E7%94%B7-2021-7-25/"><span class="level-item">损失函数 - 最小二乘与极大似然估计</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level" style="margin-bottom:1rem"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-96x96 mx-auto mb-2"><img class="avatar" src="/img/txx.png" alt="钟善男"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">钟善男</p><p class="is-size-6 is-block">zhongsn_work@163.com</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou, China</span></p></div></div></nav><nav class="level menu-list is-mobile" style="margin-bottom:1rem"><a class="level-item has-text-centered is-marginless" href="/archives/"><div><p class="heading">文章</p><div><p class="title">9</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/categories/"><div><p class="heading">分类</p><div><p class="title">2</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/tags/"><div><p class="heading">标签</p><div><p class="title">18</p></div></div></a></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://www.github.com/crushr" target="_blank" rel="noopener"><i class="fab fa-github"></i>  关注我</a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#引入"><span class="level-left"><span class="level-item">1</span><span class="level-item"> 引入 </span></span></a></li><li><a class="level is-mobile" href="#熵的概念"><span class="level-left"><span class="level-item">2</span><span class="level-item"> 熵的概念 </span></span></a></li><li><a class="level is-mobile" href="#从信息量推导熵"><span class="level-left"><span class="level-item">3</span><span class="level-item"> 从信息量推导熵 </span></span></a></li><li><a class="level is-mobile" href="#由相对熵 -KL 散度解释交叉熵为什么能代表损失函数"><span class="level-left"><span class="level-item">4</span><span class="level-item"> 由相对熵 /KL 散度解释交叉熵为什么能代表损失函数 </span></span></a></li><li><a class="level is-mobile" href="#将交叉熵应用到神经网络中"><span class="level-left"><span class="level-item">5</span><span class="level-item"> 将交叉熵应用到神经网络中 </span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo1.png" alt="勿以浮沙筑高楼" height="28"></a><p class="is-size-7"><span>&copy; 2021 钟善男</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="fab fa-creative-commons"></i> <i class="fab fa-creative-commons-by"></i> <i class="fab fa-creative-commons-nc"></i> <i class="fab fa-creative-commons-sa"></i> </a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="AlphaLxy GitHub" href="https://www.github.com/crushr"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.css"><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/katex.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/KaTeX/0.11.1/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>